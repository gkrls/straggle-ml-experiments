+ exec python -u /home/gks/straggle-ml-experiments/models/bert_finetune.py --rank 0 --world_size 6 --iface ens4f0 --master_addr 42.0.0.1 --master_port 29500 --backend gloo --data /home/gks/datasets/squad_v2 --epochs 5 --batch_size 32 --learning_rate 3e-5 --warmup_ratio 0.1 --deterministic --drop_last_val --workers 8 --prefetch_factor 4 --json /home/gks/straggle-ml-experiments/models/bert_finetune.json
[DDP] backend=gloo world_size=6 master=42.0.0.1:29500 iface=ens4f0 local_rank=0
{
  "rank": 0,
  "world_size": 6,
  "iface": "ens4f0",
  "master_addr": "42.0.0.1",
  "master_port": 29500,
  "backend": "gloo",
  "device": "cuda",
  "deterministic": true,
  "workers": 8,
  "model_name": "bert-base-uncased",
  "data": "/home/gks/datasets/squad_v2",
  "force_download": false,
  "squad_version": "v1",
  "epochs": 5,
  "batch_size": 32,
  "learning_rate": 3e-05,
  "weight_decay": 0.0,
  "warmup_ratio": 0.1,
  "amp": false,
  "drop_last_train": false,
  "drop_last_val": true,
  "static_graph": false,
  "prefetch_factor": 4,
  "max_grad_norm": 1.0,
  "log_interval": 0,
  "max_seq_len": 384,
  "doc_stride": 128,
  "max_answer_length": 30,
  "n_best_size": 20,
  "null_score_diff_threshold": 0.0,
  "straggle_points": 0,
  "straggle_prob": 0,
  "straggle_ranks": [],
  "straggle_amount": 0,
  "straggle_multiply": [
    1.0,
    1.0
  ],
  "straggle_verbose": false,
  "json": "/home/gks/straggle-ml-experiments/models/bert_finetune.json",
  "local_rank": 0,
  "seed": 42
}
[Data] Loading 'squad' under /home/gks/datasets/squad_v2 (first run downloads; then reuses)
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model 'bert-base-uncased' initialized.
[warning] straggle_sim created but not active -- points: 0, prob: 0.0, amount: 0.0
Straggle sim inactive
[2025-09-07 15:39:43][Epoch 000] ...
/home/gks/straggle-ml-experiments/venv/lib/python3.12/site-packages/torch/autograd/graph.py:824: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:752.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-09-07 15:50:30][Epoch 000] train_loss=2.4297 (global=2.4502) val_loss=1.1780 val_em=73.37% val_f1=82.43% lr=0.000027 epoch_time=647.19s step_time=1.35s (min=0.71s, max=2.73s) tp=~23.7 samples/s straggle_events=0
[2025-09-07 15:50:30][Epoch 001] ...
[2025-09-07 16:01:16][Epoch 001] train_loss=1.0769 (global=1.0754) val_loss=1.0527 val_em=77.77% val_f1=85.42% lr=0.000020 epoch_time=646.12s step_time=1.35s (min=0.69s, max=1.38s) tp=~23.7 samples/s straggle_events=0
[2025-09-07 16:01:16][Epoch 002] ...
[2025-09-07 16:12:02][Epoch 002] train_loss=0.8716 (global=0.8644) val_loss=1.0296 val_em=78.49% val_f1=85.87% lr=0.000013 epoch_time=646.27s step_time=1.35s (min=0.74s, max=1.41s) tp=~23.7 samples/s straggle_events=0
[2025-09-07 16:12:02][Epoch 003] ...
[2025-09-07 16:22:50][Epoch 003] train_loss=0.7331 (global=0.7427) val_loss=1.0563 val_em=78.99% val_f1=86.23% lr=0.000007 epoch_time=647.93s step_time=1.35s (min=0.68s, max=1.41s) tp=~23.6 samples/s straggle_events=0
[2025-09-07 16:22:50][Epoch 004] ...
[2025-09-07 16:33:39][Epoch 004] train_loss=0.6797 (global=0.6729) val_loss=1.0655 val_em=79.32% val_f1=86.48% lr=0.000000 epoch_time=648.42s step_time=1.35s (min=0.71s, max=1.42s) tp=~23.6 samples/s straggle_events=0
