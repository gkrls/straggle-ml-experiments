+ exec python -u /home/gks/straggle-ml-experiments/models/bert_finetune.py --rank 0 --world_size 6 --iface ens4f0 --master_addr 42.0.0.1 --master_port 29500 --backend gloo --data /home/gks/datasets/squad_v1 --squad_version v1 --epochs 5 --batch_size 32 --learning_rate 3e-5 --warmup_ratio 0.1 --deterministic --workers 8 --prefetch_factor 4 --straggle_points 3 --straggle_prob 2 --straggle_ranks 1 --straggle_amount 1.35 --straggle_multiply 0.5 2 --straggle_verbose --json /home/gks/straggle-ml-experiments/models/bert_finetune_straggle.json
[DDP] backend=gloo world_size=6 master=42.0.0.1:29500 iface=ens4f0 local_rank=0
{
  "rank": 0,
  "world_size": 6,
  "iface": "ens4f0",
  "master_addr": "42.0.0.1",
  "master_port": 29500,
  "backend": "gloo",
  "device": "cuda",
  "deterministic": true,
  "workers": 8,
  "model_name": "bert-base-uncased",
  "data": "/home/gks/datasets/squad_v1",
  "force_download": false,
  "squad_version": "v1",
  "epochs": 5,
  "batch_size": 32,
  "learning_rate": 3e-05,
  "weight_decay": 0.0,
  "warmup_ratio": 0.1,
  "amp": false,
  "drop_last_train": false,
  "drop_last_val": false,
  "static_graph": false,
  "prefetch_factor": 4,
  "max_grad_norm": 1.0,
  "log_interval": 0,
  "max_seq_len": 384,
  "doc_stride": 128,
  "max_answer_length": 30,
  "n_best_size": 20,
  "null_score_diff_threshold": 0.0,
  "straggle_points": 3,
  "straggle_prob": 2.0,
  "straggle_ranks": [
    1
  ],
  "straggle_amount": 1.35,
  "straggle_multiply": [
    0.5,
    2.0
  ],
  "straggle_verbose": true,
  "json": "/home/gks/straggle-ml-experiments/models/bert_finetune_straggle.json",
  "local_rank": 0,
  "seed": 42
}
[Data] Loading 'squad' under /home/gks/datasets/squad_v1 (first run downloads; then reuses)
Tokenize val: 100%|██████████████| 10570/10570 [00:07<00:00, 1451.89 examples/s]
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model 'bert-base-uncased' initialized.
[straggle_sim] rank 0 not in target ranks {1}, skipping
Straggle sim inactive
[2025-09-07 19:29:40][Epoch 000] ...
/home/gks/straggle-ml-experiments/venv/lib/python3.12/site-packages/torch/autograd/graph.py:824: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:752.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-09-07 19:40:57][Epoch 000] train_loss=2.4297 (global=2.4502) val_loss=1.1782 val_em=72.40% val_f1=81.30% lr=0.000027 epoch_time=676.91s step_time=1.40s (min=0.75s, max=3.87s) tp=~22.8 samples/s straggle_events=0
[2025-09-07 19:40:57][Epoch 001] ...
[2025-09-07 19:52:01][Epoch 001] train_loss=1.0769 (global=1.0754) val_loss=1.0530 val_em=77.14% val_f1=84.62% lr=0.000020 epoch_time=664.59s step_time=1.38s (min=1.32s, max=3.66s) tp=~23.1 samples/s straggle_events=0
[2025-09-07 19:52:01][Epoch 002] ...
[2025-09-07 20:03:19][Epoch 002] train_loss=0.8716 (global=0.8644) val_loss=1.0302 val_em=78.24% val_f1=85.70% lr=0.000013 epoch_time=677.91s step_time=1.42s (min=0.69s, max=3.83s) tp=~22.5 samples/s straggle_events=0
[2025-09-07 20:03:19][Epoch 003] ...
[2025-09-07 20:14:34][Epoch 003] train_loss=0.7331 (global=0.7427) val_loss=1.0571 val_em=78.18% val_f1=85.76% lr=0.000007 epoch_time=674.39s step_time=1.40s (min=0.73s, max=3.92s) tp=~22.8 samples/s straggle_events=0
[2025-09-07 20:14:34][Epoch 004] ...
[2025-09-07 20:25:58][Epoch 004] train_loss=0.6797 (global=0.6729) val_loss=1.0663 val_em=78.30% val_f1=85.80% lr=0.000000 epoch_time=684.69s step_time=1.43s (min=0.69s, max=3.90s) tp=~22.4 samples/s straggle_events=0